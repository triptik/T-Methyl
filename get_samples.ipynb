{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "import urllib2\n",
    "import re\n",
    "from subprocess import PIPE, Popen\n",
    "import os\n",
    "import glob\n",
    "import gzip\n",
    "from itertools import islice\n",
    "import ftplib\n",
    "import math\n",
    "from sklearn import feature_selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats.stats import pearsonr\n",
    "import random\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this study, methlyation datasets were obtained from Gene Expression Omnibus (GEO) database using GSE identifiers. \n",
    "File \"geo-noncancer-datasets.txt\" and \"geo-cancer-datasets.txt\" files provides an overview of the publicly available human DNAm data sets used in this study for cancer and non-cancer cases, respectively.  \n",
    "\n",
    "Some sections of the code below will only run one GSE id for demo purpose. \n",
    "\n",
    "All the raw data and processed will be found here:\n",
    "https://drive.google.com/folderview?id=0B5edDxPaIzYHQnlWR3lGMFYtOUk&usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-cancer datasets 48\n",
      "Cancer datasets 7\n"
     ]
    }
   ],
   "source": [
    "geo_noncancer_list = []\n",
    "#read the non-cancer dataset list from tab delim file\n",
    "noncancer_df=pd.read_csv('geo-noncancer-datasets.txt',sep='\\t')\n",
    "#extract the geo identifiers\n",
    "geo_noncancer_list = list(set(noncancer_df.Availability)) \n",
    "print \"Non-cancer datasets\",len(geo_noncancer_list)\n",
    "\n",
    "#read the non-cancer dataset list from tab delim file\n",
    "cancer_df=pd.read_csv('geo-cancer-datasets.txt',sep='\\t')\n",
    "#extract the geo identifiers\n",
    "geo_cancer_list = (cancer_df.Availability).values\n",
    "print \"Cancer datasets\", len(geo_cancer_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNA methylation relevant data was available in different files. \n",
    "\n",
    "For each GSE identifier there were Series matrix files  - containing expriment metadata like tissue source, chronogical age and experiment id's. \n",
    "Signal files -  These contained methlyation signals for every biomarker  as methylated and non-methylated, from which beta values were calculated. In some cases beta values were already made available. \n",
    "\n",
    "Data format challenges - There were many challenges in terms dowloading required files as there were no standard naming conventions and in some cases internal format was also quite different. Also, in some cases methlyation signals were available throgh series files instead of signal files.\n",
    "\n",
    "Some of the edge cases had to be manually converted into a standard format in order for code to work.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE40nnn/GSE40700/matrix/GSE40700_series_matrix.txt.gz\n",
    "#below functions check whether a ftp works for gse id according to standard format and downloads them \n",
    "#Note: download commands, adding a break in for loop for demo puropose, so that it shows working for one gseid\n",
    "def check_urllink(ftplink):\n",
    "    #check if the ftp link exists and check for edge cases \n",
    "    try:\n",
    "        urllib2.urlopen(ftplink)\n",
    "        return ftplink\n",
    "    except urllib2.HTTPError, e:\n",
    "        print(e.code)\n",
    "        return 1\n",
    "    except urllib2.URLError, e:\n",
    "        print(e.args)\n",
    "        return 1\n",
    "    \n",
    "#iterate over the accessions to get series file for metadata\n",
    "#(these contain methylation signals in some cases)\n",
    "def fetch_series_data(geo_noncancer_list):\n",
    "    outdir ='data/geo-series'\n",
    "    gse_noncancer_edge=[]\n",
    "    failed_fetch_list = []\n",
    "    for i in geo_noncancer_list:\n",
    "        key_num = i.split(\"GSE\")[1]\n",
    "        url_key= \"GSE\"+key_num[0:2]\n",
    "        file_n = i + \"_series_matrix.txt.gz\"\n",
    "        url = \"ftp://ftp.ncbi.nlm.nih.gov/geo/series/\"+url_key.replace(\" \",\"\")+ \"nnn/\" + i + \"/matrix/\" \n",
    "        ftplink= (url + file_n).replace(\" \",\"\")\n",
    "        result = check_urllink(ftplink)\n",
    "        ftp_cmd= \"cd \" + outdir + \"; ftp \" + ftplink + \" ;cd -\"\n",
    "        if result == 1:\n",
    "            print \"link failure, cannot find series files\", ftplink\n",
    "            gse_noncancer_edge.append(i)\n",
    "        else:\n",
    "            print \"link Success: Fetching \", ftplink\n",
    "            #below part triggers ftp download cmd in shell using subprocess\n",
    "            pfetch = Popen(ftp_cmd,stdout=PIPE,stderr=PIPE,shell=True,close_fds=True)\n",
    "            std_out, std_err = pfetch.communicate()\n",
    "            exit_code = pfetch.returncode\n",
    "            if exit_code:\n",
    "                failed_fetch_list.append(i)\n",
    "            #os.system(ftp_cmd)\n",
    "        #adding break here, just so that it works for one accession\n",
    "        break\n",
    "    return gse_noncancer_edge, failed_fetch_list \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link Success: Fetching  ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE28nnn/GSE28746/matrix/GSE28746_series_matrix.txt.gz\n"
     ]
    }
   ],
   "source": [
    "#download noncancer datasets\n",
    "gse_noncancer_edge, failed_fetch_list = fetch_series_data(geo_noncancer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link Success: Fetching  ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE31nnn/GSE31979/matrix/GSE31979_series_matrix.txt.gz\n"
     ]
    }
   ],
   "source": [
    "#download series files for cancer datasets\n",
    "gse_cancer_edge, failed_fetch_list = fetch_series_data(geo_cancer_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of GEO non-cancer edge cases 0\n",
      "number of failed fetches for normal GEO data links 0\n"
     ]
    }
   ],
   "source": [
    "print \"number of GEO non-cancer edge cases\", len(gse_noncancer_edge)\n",
    "print \"number of failed fetches for normal GEO data links\" , len(failed_fetch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier we downloaded series files (~6gb compressed series files), now below functions will get signal files (when done these were ~5gb compressed), \n",
    "signal files contain methylation and non methylation signals, which have to be converted into beta values\n",
    "More about betavalues in methylation can be found here: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3012676/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ftp_listfiles(ftplink):\n",
    "    filelist = []\n",
    "    urlpath = urllib2.urlopen(ftplink)\n",
    "    string = urlpath.read().decode('utf-8').encode()\n",
    "    for txt in string.split(\"\\n\"):\n",
    "        f = txt.split(\" \")[-1]\n",
    "        if re.search(\"signal\", f) or re.search(\"Signal\", f):\n",
    "            filelist.append(f)\n",
    "        elif re.search(\"normalize\",f) or re.search(\"Normalize\",f) :\n",
    "            filelist.append(f)\n",
    "    return filelist\n",
    "\n",
    "def fetch_signalfiles(geo_list):\n",
    "    gse_edge=[]\n",
    "    failed_fetch_list = []\n",
    "    outdir = 'data/geo-signals'\n",
    "    for i in geo_list:\n",
    "        i = i.replace(\" \",'')\n",
    "        key_num = i.split(\"GSE\")[1]\n",
    "        url_key= \"GSE\"+key_num[0:2]\n",
    "        url = \"ftp://ftp.ncbi.nlm.nih.gov/geo/series/\"+url_key+ \"nnn/\" + i + \"/suppl/\"\n",
    "        files = ftp_listfiles(url)\n",
    "        for file_n in files:\n",
    "            ftplink= (url + file_n).replace(\" \",\"\").replace(\"\\r\",'')\n",
    "            result=check_urllink(ftplink)\n",
    "            ftp_cmd= \"cd \" + outdir + \"; ftp \" + ftplink + \" ;cd -\"\n",
    "            if result == 1:\n",
    "                print \"link failure, cannot find signal file\", ftplink\n",
    "                gse_edge.append(i)\n",
    "            else:\n",
    "                print \"Link Success: Fetching \", ftplink\n",
    "                pfetch = Popen(ftp_cmd,stdout=PIPE,stderr=PIPE,shell=True,close_fds=True)\n",
    "                std_out, std_err = pfetch.communicate()\n",
    "                exit_code = pfetch.returncode\n",
    "                if exit_code:\n",
    "                    failed_fetch_list.append(i)\n",
    "                #os.system(ftp_cmd)\n",
    "            #adding break for demo purpose\n",
    "        break\n",
    "    return gse_edge, failed_fetch_list  \n",
    "\n",
    "def beta_val(df):\n",
    "    #Max(M,0)/[Max(M,0)+Max(U,0)+100]. \n",
    "    #beta val = methylated signal /(unmethylated signal + methylated signal + 100)\n",
    "    #Thus, Î² values range from 0 (completely un-methylated) to 1 (completely methylated) . \n",
    "    return (float(df[1])/float(df[1] + df[0]+100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link Success: Fetching  ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE28nnn/GSE28746/suppl/GSE28746_non-normalized.txt.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_resem, failed = fetch_signalfiles(geo_noncancer_list)\n",
    "len (signal_resem), len(failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link Success: Fetching  ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE31nnn/GSE31979/suppl/GSE31979_matrix_signal_intensities.txt.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal_resem, failed = fetch_signalfiles(geo_cancer_list)\n",
    "len (signal_resem), len(failed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the signal files were really big, I had to chunk them and read it sequentially processed them and append. Below functions work on chunk dataframes and return beta values. There is lot of scope for optimization by using spark parallel processing.  But decided to try optimizations later and make sure the method works first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def open_signalfile(file1):\n",
    "    #accepts file and returns file chunk iterator\n",
    "    signaldf_iter = None\n",
    "    signaldf_iter=pd.read_csv(file1,sep=\"\\t\",comment=\"#\",index_col=0,iterator=True, chunksize=10000)\n",
    "    return signaldf_iter\n",
    "\n",
    "def process_chunkdf(chunk):\n",
    "    #for each chunk df calculates the beta values \n",
    "    betadf = pd.DataFrame()\n",
    "    #print chunk.shape\n",
    "    filter = []\n",
    "    #dropping rows if any of them have NaN\n",
    "    chunk.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)\n",
    "    list_cols = chunk.columns\n",
    "    #adding edge cases\n",
    "    #this takes care of remvoing unwanted columns to reduce memory usage\n",
    "    filter = ~(list_cols.str.contains(\"pval|background|probeid|symbol\",case=False, regex=True))\n",
    "    filter_cols = list_cols[filter]\n",
    "    chunk=chunk[filter_cols]\n",
    "    #print chunk.shape \n",
    "    #further check if the dataframe has beta values already.\n",
    "    filter = filter_cols.str.contains(\"beta\",case=False, regex=True)\n",
    "    #if the file contain beta values already calculated, then fetch only those columns\n",
    "    if np.sum(filter) > 0:\n",
    "        filter_cols = filter_cols[filter]\n",
    "        betadf=chunk[filter_cols]\n",
    "        #rename index for consistency \n",
    "        betadf.index.names = ['cpg_id'] \n",
    "    else:\n",
    "        #if file contains methlyated and unmethlyated then calculate beta values.\n",
    "        chunk.index.names = ['cpg_id'] \n",
    "        for i in range(len(chunk.columns))[::2]:\n",
    "            col_1 = chunk.columns[i]\n",
    "            col_2 = chunk.columns[i+1]\n",
    "            #print col_1 ,col_2\n",
    "            #create column with polished name\n",
    "            col_new = col_1.replace('Unmethylated Signal','').replace('Signal_A','')\n",
    "            betadf[col_new] = chunk[[col_1,col_2]].apply(beta_val,axis=1)\n",
    "    return betadf\n",
    "\n",
    "def create_betasig_df(iterator):\n",
    "    #takes file chunk iterater and returns betavalues df for full file\n",
    "    finaldf = pd.DataFrame()\n",
    "    i = 0\n",
    "    for chunk in iterator:\n",
    "        i= i +1\n",
    "        #print chunk.shape\n",
    "        betadf = process_chunkdf(chunk)\n",
    "        finaldf = pd.concat([finaldf, betadf])\n",
    "    return finaldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_signal_methyldf(filearr):\n",
    "    #iterates over signal files , get chunk iterators , beta values and writes them out to file\n",
    "    outputpath = 'data/geo-betasignals/'\n",
    "    trouble_samples = []\n",
    "    for file1 in filearr:\n",
    "        print file1\n",
    "        tmpdf=pd.DataFrame()\n",
    "        signaldf_iter = ''\n",
    "        try:\n",
    "            signaldf_iter = open_signalfile(file1)\n",
    "            tmpdf = create_betasig_df(signaldf_iter)\n",
    "        except Exception as e:\n",
    "            trouble_samples.append(file1)\n",
    "            continue\n",
    "        #create output file name for betavalues\n",
    "        path,filename = os.path.split(file1)\n",
    "        filename, ext = os.path.splitext(filename)\n",
    "        filename, ext = os.path.splitext(filename)\n",
    "        tmpdf.to_csv(outputpath+filename+'.csv',sep=',')\n",
    "    return trouble_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the beta value calculation functionality on each files and capture edge cases as trouble_samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/geo-signals/GSE28746_non-normalized.txt.gz\n",
      "data/geo-signals/GSE31979_matrix_signal_intensities.txt.gz\n",
      "0\n",
      "CPU times: user 6min 37s, sys: 5.85 s, total: 6min 43s\n",
      "Wall time: 6min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#demo block\n",
    "filearr= glob.glob('data/geo-signals/'+'*.txt.gz')\n",
    "trouble_samples = get_signal_methyldf(filearr)\n",
    "print len(trouble_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#full run, commenting out to avoid recalculation\n",
    "filearr= glob.glob('data/geo-signals/'+'*.txt.gz')\n",
    "#trouble_samples = get_signal_methyldf(filearr)\n",
    "edge_case = ['GSE20236','GSE26033','GSE30090','GSE30780','GSE32146','GSE34257','GSE34639','GSE36064','GSE43269']\n",
    "len(edge_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Had to manually curate the edge case signal files as the column structure was random.\n",
    "There was no consistency or pattern among these to customize the code, moved them under \"geo-nonuniform-signalfiles\" dir and processed files to have - cpgid, non-methylation column, methylation column, pval(optional). \n",
    "And, repeated the beta val calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trouble_samples=glob.glob('data/geo-nonuniform-signalfiles/*.txt.gz')\n",
    "#commenting out to avoid recalcualtion\n",
    "#trouble_samples = get_signal_methyldf(trouble_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some of the accessions did not have signal files, instead series files contained the beta values. \n",
    "We will deal with them later. Below function takes care of just reading series file into df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_series_methyldf(file1):\n",
    "    #read series file, skip comment line and return the df as methylation df\n",
    "    maindf=pd.read_csv(file1,comment=\"!\",sep=\"\\t\",index_col=0,skip_blank_lines=True)\n",
    "    return maindf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gse_without_signals=['GSE17448','GSE20067','GSE30758','GSE32393','GSE41169','GSE42510','GSE40700','GSE32149','GSE19711','GSE20712','GSE25062']\n",
    "for i in gse_without_signals:\n",
    "    arr = glob.glob( 'data/geo-series/'+ i+ \"*.txt.gz\")\n",
    "    for file1 in arr:\n",
    "        tmpdf = create_series_methyldf(file1)\n",
    "        path,filename = os.path.split(file1)\n",
    "        filename, ext = os.path.splitext(filename)\n",
    "        filename, ext = os.path.splitext(filename)\n",
    "        tmpdf.to_csv(outputpath+filename+'.csv',sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below section takes care of downloading metadata from series files for every sample\n",
    "Metadata includes : chronological age, gender , tissue source, sample GEO id, sample id and if cancer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_metadata(file1,cancer=0):\n",
    "    #approx number of lines to fetch tissue information\n",
    "    N = 2000\n",
    "    f = gzip.open(file1, 'rb')\n",
    "    #Reading first 2000 lines, header lines are inconsistent but cannot contain such large number. \n",
    "    headerlines = islice(f, N)\n",
    "    metadict = {'sample_geo_id':[],'age':[],'gender':[],'sample_id':[],'cancer':[]}\n",
    "    for i in headerlines:\n",
    "        line = i.strip(\"\\n\")\n",
    "        if re.search('!Sample_geo_accession',line,re.IGNORECASE):\n",
    "            metadict[\"sample_geo_id\"] = line.replace('\"','').split(\"\\t\")[1:]\n",
    "            ref_len = len( metadict[\"sample_geo_id\"] )\n",
    "            #intialize metadata array lengths according to geo ref lengths\n",
    "            #metadict[\"sample_id\"] = metadict[\"sample_geo_id\"]\n",
    "            metadict[\"tissue_src\"] = [np.nan]*ref_len\n",
    "            metadict[\"age\"] = [np.nan]*ref_len\n",
    "            metadict[\"gender\"] = [np.nan]*ref_len\n",
    "            metadict[\"cancer\"] = [cancer]*ref_len\n",
    "        if re.search('!Sample_source_name_ch1',line,re.IGNORECASE):\n",
    "            metadict[\"tissue_src\"] = line.replace('\"','').split(\"\\t\")[1:]\n",
    "        if re.search('!Sample_title',line,re.IGNORECASE) :\n",
    "            metadict[\"sample_id\"] = line.replace('\"','').split(\"\\t\")[1:]\n",
    "        if re.search('!Sample_characteristics_ch1',line, re.IGNORECASE):\n",
    "            #iterate over each line characteristics line to fetch age, gender info\n",
    "            temp_arr = line.split(\"\\t\")[1:]\n",
    "            for l in range(0,len(temp_arr)):\n",
    "                ele = temp_arr[l]\n",
    "                if re.search('ageatdiagnosis',ele, re.IGNORECASE) or re.search('\"age',ele, re.IGNORECASE):\n",
    "                    #check for months , default is years\n",
    "                    ele = ele.replace('\"','')\n",
    "                    years = re.sub(\"[^0-9]\", \"\", ele)\n",
    "                    if re.search('month',ele,re.IGNORECASE):\n",
    "                        years = float(years)/12.0\n",
    "                    metadict[\"age\"][l] = years\n",
    "                if re.search('sex', ele, re.IGNORECASE) or re.search('gender', ele, re.IGNORECASE):\n",
    "                    metadict[\"gender\"][l] = ele.replace('gender','').replace('sex','').replace(':','').replace(' ','').replace('\"','')\n",
    "    metadatadf = pd.DataFrame(metadict)\n",
    "    return metadatadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 445 ms, sys: 9.51 ms, total: 455 ms\n",
      "Wall time: 475 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "datasets = glob.glob(\"data/geo-series/*txt.gz\")\n",
    "outputpath = 'data/geo-metadata/'\n",
    "for file1 in datasets:\n",
    "    #create output file name\n",
    "    path,filename = os.path.split(file1)\n",
    "    filename, ext = os.path.splitext(filename)\n",
    "    filename, ext = os.path.splitext(filename)\n",
    "    #check if the dataset is from cancer samples\n",
    "    if (filename.split(\"_\")[0]).replace(\"\\t\",'') in geo_cancer_list:\n",
    "        flag= 1\n",
    "    else:\n",
    "        flag = 0\n",
    "    metadf = create_metadata(file1,cancer=flag)\n",
    "    metadf.to_csv(outputpath+filename+'_meta.csv',sep=',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since datasets were obtained from different platforms, it was curical for us to normalize the data for signals to be comparable. The Illumina 450K platforms uses two different chemical assays. We used R method from Teschendorff et al 2013 based on a intra-array normalization strategy for the 450K platform, called BMIQ (Beta MIxture Quantile dilation), which adjusts beta-values of type II probes into a statistical distribution characteristic of type I probes.  \n",
    "\n",
    "Reference for normalization : Teschendorff AE, Marabita F, Lechner M, Bartlett T, Tegner J, Gomez-Cabrero D, Beck S: A beta-mixture quantile normalization method for correcting probe design bias in Illumina Infinium 450 k DNA methylation data. Bioinformatics 2013, 29:189-196.\n",
    "http://www.ncbi.nlm.nih.gov/pubmed/23175756"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normfailed=[]\n",
    "def normalize_datasets(file1):\n",
    "        outputpath = 'data/geo-normalized/'\n",
    "        probeannotfile = 'probeAnnotation21kdatMethUsed.csv'\n",
    "        #create output file name for betavalues\n",
    "        path,filename = os.path.split(file1)\n",
    "        filename, ext = os.path.splitext(filename)\n",
    "        filename, ext = os.path.splitext(filename)\n",
    "        outfile = outputpath + \"/\" + filename +\"_norm.csv\"\n",
    "        norm_cmd = \"Rscript  Normalization.R \" + file1 + \" \" + probeannotfile + \" \" + outfile\n",
    "        pnorm = Popen(norm_cmd,stdout=PIPE,stderr=PIPE,shell=True,close_fds=True)\n",
    "        std_out, std_err = pnorm.communicate()\n",
    "        exit_code = pnorm.returncode\n",
    "        if exit_code:\n",
    "            print \"normalization error\", file1\n",
    "            normfailed.append(file1+ \":\"+norm_cmd )\n",
    "        return normfailed        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalization error data/geo-betasignals/GSE28746_non-normalized.csv\n",
      "CPU times: user 1.67 ms, sys: 5.55 ms, total: 7.22 ms\n",
      "Wall time: 5.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filearr= glob.glob('data/geo-betasignals/'+'*.csv')\n",
    "for file1 in sorted(filearr):\n",
    "    path,f = os.path.split(file1)\n",
    "    failnorm_files = normalize_datasets(file1)\n",
    "    #adding break to iterate over only one file for demo\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our studies only 21k probes will retained and sample id will be replaced with geo accessions.\n",
    "Also, age, tissue_src and other metadata will be added to normalized dataframe to final df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verify data data/geo-normalized/GSE28746_non-normalized_norm.csv\n",
      "exception  list index out of range\n",
      "Breast\n"
     ]
    }
   ],
   "source": [
    "#extract cpgid for 21k probes\n",
    "probdf=pd.read_csv('probeAnnotation21kdatMethUsed.csv')\n",
    "#extract tissue information for all geo-accessions, cancer and non-cancer\n",
    "standard_tissuedf = pd.read_csv(\"geo-accession-tissueinfo.txt\", sep=\"\\t\")\n",
    "#reading the file for tissue info for each geo accession\n",
    "tissue_list = list(standard_tissuedf[\"Availability\"].values)\n",
    "#fetch normalized files \n",
    "normalized_files = glob.glob('data/geo-normalized/*_norm.csv')\n",
    "metadir = 'data/geo-metadata/'\n",
    "preprocess_outdir = 'data/geo-preprocessed/'\n",
    "for file1 in normalized_files:\n",
    "    \"\"\"iterate over normalized files and based on filename accession\n",
    "    fetch metadata and merge them. Since tissue_src line is too lengthy\n",
    "    we will use shortened versions which we read from standard-tissuedf\"\"\"\n",
    "    try:\n",
    "        path,filename = os.path.split(file1)\n",
    "        gse_id = filename.split(\"_\")[0]\n",
    "        metafile = glob.glob(metadir + '/' + gse_id + \"*\")[0] \n",
    "        metadf = pd.read_csv(metafile)\n",
    "        #get standard tissue name and replace in metadf \n",
    "        #removing the tissue_src information with standard tissue notations. \n",
    "        #for example  : \"uterine cervix\" instead of \"genomic dna from uterine cervix\"\n",
    "        if gse_id in tissue_list:\n",
    "            idx = tissue_list.index(gse_id)\n",
    "            tissue_name = standard_tissuedf[\"DNA origin\"][idx]   \n",
    "            metadf[\"tissue_src\"] = [tissue_name] * metadf.shape[0]\n",
    "        print tissue_name\n",
    "        normdf = pd.read_csv(file1,index_col=0)\n",
    "        normdf.reset_index(inplace=True)\n",
    "        normdf.set_index(\"index\",inplace=True)\n",
    "        normdf.columns = list(probdf[\"Name\"])\n",
    "        normdf.reset_index(inplace=True)\n",
    "        if re.search(\"GS\",normdf[\"index\"][0]):\n",
    "            normdf.rename(columns={\"index\":\"sample_geo_id\"}, inplace=True)\n",
    "        else:\n",
    "            normdf.rename(columns={\"index\":\"sample_id\"}, inplace=True)\n",
    "        resultdf = pd.concat([normdf, metadf], axis=1, join_axes=[normdf.index])\n",
    "        resultdf = resultdf.T.groupby(level=0).first().T\n",
    "        resultdf.to_csv(preprocess_outdir+filename.replace(\"_norm.csv\",\"_final.csv.gz\"),sep=',', index=False)\n",
    "    except Exception as e:\n",
    "        print \"verify data\", file1\n",
    "        print \"exception \", e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MSC (bonemarrow)', 'Blood WB', 'Blood WB', 'Blood WB', 'Blood WB',\n",
       "       'Buccal', 'Blood CD4+CD14', 'Dermal fibroblast', 'Buccal', 'Heart',\n",
       "       'Chimp+Human Tissues', 'Prostate NL', 'Sperm', 'Blood PBMC',\n",
       "       'Blood Cord', 'Saliva', 'HematopoieticStemAndNormalPrimaryTissue',\n",
       "       'hESC                                      ', 'Gastric',\n",
       "       'Stem cells+Somatic Cells', 'Uterine Cervix', 'Blood PBMC',\n",
       "       'Stem cells+Somatic Cells', 'Colon', 'Blood PBMC', 'Breast NL',\n",
       "       'Saliva', 'Saliva', 'Blood Cord', 'Blood CD4 Tcells',\n",
       "       'Blood Cell Types', 'Blood PBMC', 'Muscle', 'Blood Cord',\n",
       "       'Blood PBMC', 'Reprogrammed mesenchymal stromal cells ', 'Liver ',\n",
       "       'Fat Adip', 'Muscle', 'Brain Cerebellar', 'Brain Occipital Cortex',\n",
       "       'Brain CRBLM', 'Various Tissues', 'Blood WB', 'Blood WB', 'Ape WB',\n",
       "       'BrainVariousCells', 'Heart', 'Buccal', 'Buccal',\n",
       "       'Blood Cell Types', 'Cartilage Knee', 'Placenta', 'Sperm', 'Brain',\n",
       "       'Breast', 'Colorectal', 'Prostate', 'Breast', 'Breast', 'Urothelium'], dtype=object)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_tissuedf[\"DNA origin\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
